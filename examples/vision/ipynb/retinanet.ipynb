{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGTlLOc0u13C"
      },
      "source": [
        "# Object Detection with RetinaNet\n",
        "\n",
        "**Author:** [Srihari Humbarwadi](https://twitter.com/srihari_rh)<br>\n",
        "**Date created:** 2020/05/17<br>\n",
        "**Last modified:** 2023/07/10<br>\n",
        "**Description:** Implementing RetinaNet: Focal Loss for Dense Object Detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64MxmXeYu13G"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Object detection a very important problem in computer\n",
        "vision. Here the model is tasked with localizing the objects present in an\n",
        "image, and at the same time, classifying them into different categories.\n",
        "Object detection models can be broadly classified into \"single-stage\" and\n",
        "\"two-stage\" detectors. Two-stage detectors are often more accurate but at the\n",
        "cost of being slower. Here in this example, we will implement RetinaNet,\n",
        "a popular single-stage detector, which is accurate and runs fast.\n",
        "RetinaNet uses a feature pyramid network to efficiently detect objects at\n",
        "multiple scales and introduces a new loss, the Focal loss function, to alleviate\n",
        "the problem of the extreme foreground-background class imbalance.\n",
        "\n",
        "**References:**\n",
        "\n",
        "- [RetinaNet Paper](https://arxiv.org/abs/1708.02002)\n",
        "- [Feature Pyramid Network Paper](https://arxiv.org/abs/1612.03144)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Z5o29LA2u13I"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPzgqGPnu13K"
      },
      "source": [
        "## Downloading the COCO2017 dataset\n",
        "\n",
        "Training on the entire COCO2017 dataset which has around 118k images takes a\n",
        "lot of time, hence we will be using a smaller subset of ~500 images for\n",
        "training in this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-JQ0TOWVu13M"
      },
      "outputs": [],
      "source": [
        "url = \"https://github.com/srihari-humbarwadi/datasets/releases/download/v0.1.0/data.zip\"\n",
        "filename = os.path.join(os.getcwd(), \"data.zip\")\n",
        "keras.utils.get_file(filename, url)\n",
        "\n",
        "\n",
        "with zipfile.ZipFile(\"data.zip\", \"r\") as z_fp:\n",
        "    z_fp.extractall(\"./\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc6F7-4ou13P"
      },
      "source": [
        "## Implementing utility functions\n",
        "\n",
        "Bounding boxes can be represented in multiple ways, the most common formats are:\n",
        "\n",
        "- Storing the coordinates of the corners `[xmin, ymin, xmax, ymax]`\n",
        "- Storing the coordinates of the center and the box dimensions\n",
        "`[x, y, width, height]`\n",
        "\n",
        "Since we require both formats, we will be implementing functions for converting\n",
        "between the formats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4eEOUx6Ju13Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "def swap_xy(boxes):\n",
        "    \"\"\"Swaps order the of x and y coordinates of the boxes.\n",
        "\n",
        "    Arguments:\n",
        "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes.\n",
        "\n",
        "    Returns:\n",
        "      swapped boxes with shape same as that of boxes.\n",
        "    \"\"\"\n",
        "    return tf.stack([boxes[:, 1], boxes[:, 0], boxes[:, 3], boxes[:, 2]], axis=-1)\n",
        "\n",
        "\n",
        "def convert_to_xywh(boxes):\n",
        "    \"\"\"Changes the box format to center, width and height.\n",
        "\n",
        "    Arguments:\n",
        "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
        "        representing bounding boxes where each box is of the format\n",
        "        `[xmin, ymin, xmax, ymax]`.\n",
        "\n",
        "    Returns:\n",
        "      converted boxes with shape same as that of boxes.\n",
        "    \"\"\"\n",
        "    return tf.concat(\n",
        "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n",
        "        axis=-1,\n",
        "    )\n",
        "\n",
        "\n",
        "def convert_to_corners(boxes):\n",
        "    \"\"\"Changes the box format to corner coordinates\n",
        "\n",
        "    Arguments:\n",
        "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
        "        representing bounding boxes where each box is of the format\n",
        "        `[x, y, width, height]`.\n",
        "\n",
        "    Returns:\n",
        "      converted boxes with shape same as that of boxes.\n",
        "    \"\"\"\n",
        "    return tf.concat(\n",
        "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n",
        "        axis=-1,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Afxmc-kXu13R"
      },
      "source": [
        "## Computing pairwise Intersection Over Union (IOU)\n",
        "\n",
        "As we will see later in the example, we would be assigning ground truth boxes\n",
        "to anchor boxes based on the extent of overlapping. This will require us to\n",
        "calculate the Intersection Over Union (IOU) between all the anchor\n",
        "boxes and ground truth boxes pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GA5uYHtju13S"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_iou(boxes1, boxes2):\n",
        "    \"\"\"Computes pairwise IOU matrix for given two sets of boxes\n",
        "\n",
        "    Arguments:\n",
        "      boxes1: A tensor with shape `(N, 4)` representing bounding boxes\n",
        "        where each box is of the format `[x, y, width, height]`.\n",
        "        boxes2: A tensor with shape `(M, 4)` representing bounding boxes\n",
        "        where each box is of the format `[x, y, width, height]`.\n",
        "\n",
        "    Returns:\n",
        "      pairwise IOU matrix with shape `(N, M)`, where the value at ith row\n",
        "        jth column holds the IOU between ith box and jth box from\n",
        "        boxes1 and boxes2 respectively.\n",
        "    \"\"\"\n",
        "    boxes1_corners = convert_to_corners(boxes1)\n",
        "    boxes2_corners = convert_to_corners(boxes2)\n",
        "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
        "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n",
        "    intersection = tf.maximum(0.0, rd - lu)\n",
        "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
        "    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
        "    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
        "    union_area = tf.maximum(\n",
        "        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n",
        "    )\n",
        "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)\n",
        "\n",
        "\n",
        "def visualize_detections(\n",
        "    image, boxes, classes, scores, figsize=(7, 7), linewidth=1, color=[0, 0, 1]\n",
        "):\n",
        "    \"\"\"Visualize Detections\"\"\"\n",
        "    image = np.array(image, dtype=np.uint8)\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(image)\n",
        "    ax = plt.gca()\n",
        "    for box, _cls, score in zip(boxes, classes, scores):\n",
        "        text = \"{}: {:.2f}\".format(_cls, score)\n",
        "        x1, y1, x2, y2 = box\n",
        "        w, h = x2 - x1, y2 - y1\n",
        "        patch = plt.Rectangle(\n",
        "            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
        "        )\n",
        "        ax.add_patch(patch)\n",
        "        ax.text(\n",
        "            x1,\n",
        "            y1,\n",
        "            text,\n",
        "            bbox={\"facecolor\": color, \"alpha\": 0.4},\n",
        "            clip_box=ax.clipbox,\n",
        "            clip_on=True,\n",
        "        )\n",
        "    plt.show()\n",
        "    return ax\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrPVXrusu13V"
      },
      "source": [
        "## Implementing Anchor generator\n",
        "\n",
        "Anchor boxes are fixed sized boxes that the model uses to predict the bounding\n",
        "box for an object. It does this by regressing the offset between the location\n",
        "of the object's center and the center of an anchor box, and then uses the width\n",
        "and height of the anchor box to predict a relative scale of the object. In the\n",
        "case of RetinaNet, each location on a given feature map has nine anchor boxes\n",
        "(at three scales and three ratios)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ex7JJQgZu13W"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AnchorBox:\n",
        "    \"\"\"Generates anchor boxes.\n",
        "\n",
        "    This class has operations to generate anchor boxes for feature maps at\n",
        "    strides `[8, 16, 32, 64, 128]`. Where each anchor each box is of the\n",
        "    format `[x, y, width, height]`.\n",
        "\n",
        "    Attributes:\n",
        "      aspect_ratios: A list of float values representing the aspect ratios of\n",
        "        the anchor boxes at each location on the feature map\n",
        "      scales: A list of float values representing the scale of the anchor boxes\n",
        "        at each location on the feature map.\n",
        "      num_anchors: The number of anchor boxes at each location on feature map\n",
        "      areas: A list of float values representing the areas of the anchor\n",
        "        boxes for each feature map in the feature pyramid.\n",
        "      strides: A list of float value representing the strides for each feature\n",
        "        map in the feature pyramid.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.aspect_ratios = [0.5, 1.0, 2.0]\n",
        "        self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\n",
        "\n",
        "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
        "        self._strides = [2 ** i for i in range(3, 8)]\n",
        "        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]\n",
        "        self._anchor_dims = self._compute_dims()\n",
        "\n",
        "    def _compute_dims(self):\n",
        "        \"\"\"Computes anchor box dimensions for all ratios and scales at all levels\n",
        "        of the feature pyramid.\n",
        "        \"\"\"\n",
        "        anchor_dims_all = []\n",
        "        for area in self._areas:\n",
        "            anchor_dims = []\n",
        "            for ratio in self.aspect_ratios:\n",
        "                anchor_height = tf.math.sqrt(area / ratio)\n",
        "                anchor_width = area / anchor_height\n",
        "                dims = tf.reshape(\n",
        "                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n",
        "                )\n",
        "                for scale in self.scales:\n",
        "                    anchor_dims.append(scale * dims)\n",
        "            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n",
        "        return anchor_dims_all\n",
        "\n",
        "    def _get_anchors(self, feature_height, feature_width, level):\n",
        "        \"\"\"Generates anchor boxes for a given feature map size and level\n",
        "\n",
        "        Arguments:\n",
        "          feature_height: An integer representing the height of the feature map.\n",
        "          feature_width: An integer representing the width of the feature map.\n",
        "          level: An integer representing the level of the feature map in the\n",
        "            feature pyramid.\n",
        "\n",
        "        Returns:\n",
        "          anchor boxes with the shape\n",
        "          `(feature_height * feature_width * num_anchors, 4)`\n",
        "        \"\"\"\n",
        "        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n",
        "        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n",
        "        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - 3]\n",
        "        centers = tf.expand_dims(centers, axis=-2)\n",
        "        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
        "        dims = tf.tile(\n",
        "            self._anchor_dims[level - 3], [feature_height, feature_width, 1, 1]\n",
        "        )\n",
        "        anchors = tf.concat([centers, dims], axis=-1)\n",
        "        return tf.reshape(\n",
        "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
        "        )\n",
        "\n",
        "    def get_anchors(self, image_height, image_width):\n",
        "        \"\"\"Generates anchor boxes for all the feature maps of the feature pyramid.\n",
        "\n",
        "        Arguments:\n",
        "          image_height: Height of the input image.\n",
        "          image_width: Width of the input image.\n",
        "\n",
        "        Returns:\n",
        "          anchor boxes for all the feature maps, stacked as a single tensor\n",
        "            with shape `(total_anchors, 4)`\n",
        "        \"\"\"\n",
        "        anchors = [\n",
        "            self._get_anchors(\n",
        "                tf.math.ceil(image_height / 2 ** i),\n",
        "                tf.math.ceil(image_width / 2 ** i),\n",
        "                i,\n",
        "            )\n",
        "            for i in range(3, 8)\n",
        "        ]\n",
        "        return tf.concat(anchors, axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyjFJsIMu13Y"
      },
      "source": [
        "## Preprocessing data\n",
        "\n",
        "Preprocessing the images involves two steps:\n",
        "\n",
        "- Resizing the image: Images are resized such that the shortest size is equal\n",
        "to 800 px, after resizing if the longest side of the image exceeds 1333 px,\n",
        "the image is resized such that the longest size is now capped at 1333 px.\n",
        "- Applying augmentation: Random scale jittering  and random horizontal flipping\n",
        "are the only augmentations applied to the images.\n",
        "\n",
        "Along with the images, bounding boxes are rescaled and flipped if required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MQO1d6bmu13Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "def random_flip_horizontal(image, boxes):\n",
        "    \"\"\"Flips image and boxes horizontally with 50% chance\n",
        "\n",
        "    Arguments:\n",
        "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
        "        image.\n",
        "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes,\n",
        "        having normalized coordinates.\n",
        "\n",
        "    Returns:\n",
        "      Randomly flipped image and boxes\n",
        "    \"\"\"\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        image = tf.image.flip_left_right(image)\n",
        "        boxes = tf.stack(\n",
        "            [1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]], axis=-1\n",
        "        )\n",
        "    return image, boxes\n",
        "\n",
        "\n",
        "def resize_and_pad_image(\n",
        "    image, min_side=800.0, max_side=1333.0, jitter=[640, 1024], stride=128.0\n",
        "):\n",
        "    \"\"\"Resizes and pads image while preserving aspect ratio.\n",
        "\n",
        "    1. Resizes images so that the shorter side is equal to `min_side`\n",
        "    2. If the longer side is greater than `max_side`, then resize the image\n",
        "      with longer side equal to `max_side`\n",
        "    3. Pad with zeros on right and bottom to make the image shape divisible by\n",
        "    `stride`\n",
        "\n",
        "    Arguments:\n",
        "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
        "        image.\n",
        "      min_side: The shorter side of the image is resized to this value, if\n",
        "        `jitter` is set to None.\n",
        "      max_side: If the longer side of the image exceeds this value after\n",
        "        resizing, the image is resized such that the longer side now equals to\n",
        "        this value.\n",
        "      jitter: A list of floats containing minimum and maximum size for scale\n",
        "        jittering. If available, the shorter side of the image will be\n",
        "        resized to a random value in this range.\n",
        "      stride: The stride of the smallest feature map in the feature pyramid.\n",
        "        Can be calculated using `image_size / feature_map_size`.\n",
        "\n",
        "    Returns:\n",
        "      image: Resized and padded image.\n",
        "      image_shape: Shape of the image before padding.\n",
        "      ratio: The scaling factor used to resize the image\n",
        "    \"\"\"\n",
        "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
        "    if jitter is not None:\n",
        "        min_side = tf.random.uniform((), jitter[0], jitter[1], dtype=tf.float32)\n",
        "    ratio = min_side / tf.reduce_min(image_shape)\n",
        "    if ratio * tf.reduce_max(image_shape) > max_side:\n",
        "        ratio = max_side / tf.reduce_max(image_shape)\n",
        "    image_shape = ratio * image_shape\n",
        "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
        "    padded_image_shape = tf.cast(\n",
        "        tf.math.ceil(image_shape / stride) * stride, dtype=tf.int32\n",
        "    )\n",
        "    image = tf.image.pad_to_bounding_box(\n",
        "        image, 0, 0, padded_image_shape[0], padded_image_shape[1]\n",
        "    )\n",
        "    return image, image_shape, ratio\n",
        "\n",
        "\n",
        "def preprocess_data(sample):\n",
        "    \"\"\"Applies preprocessing step to a single sample\n",
        "\n",
        "    Arguments:\n",
        "      sample: A dict representing a single training sample.\n",
        "\n",
        "    Returns:\n",
        "      image: Resized and padded image with random horizontal flipping applied.\n",
        "      bbox: Bounding boxes with the shape `(num_objects, 4)` where each box is\n",
        "        of the format `[x, y, width, height]`.\n",
        "      class_id: An tensor representing the class id of the objects, having\n",
        "        shape `(num_objects,)`.\n",
        "    \"\"\"\n",
        "    image = sample[\"image\"]\n",
        "    bbox = swap_xy(sample[\"objects\"][\"bbox\"])\n",
        "    class_id = tf.cast(sample[\"objects\"][\"label\"], dtype=tf.int32)\n",
        "\n",
        "    image, bbox = random_flip_horizontal(image, bbox)\n",
        "    image, image_shape, _ = resize_and_pad_image(image)\n",
        "\n",
        "    bbox = tf.stack(\n",
        "        [\n",
        "            bbox[:, 0] * image_shape[1],\n",
        "            bbox[:, 1] * image_shape[0],\n",
        "            bbox[:, 2] * image_shape[1],\n",
        "            bbox[:, 3] * image_shape[0],\n",
        "        ],\n",
        "        axis=-1,\n",
        "    )\n",
        "    bbox = convert_to_xywh(bbox)\n",
        "    return image, bbox, class_id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbImh2FBu13Z"
      },
      "source": [
        "## Encoding labels\n",
        "\n",
        "The raw labels, consisting of bounding boxes and class ids need to be\n",
        "transformed into targets for training. This transformation consists of\n",
        "the following steps:\n",
        "\n",
        "- Generating anchor boxes for the given image dimensions\n",
        "- Assigning ground truth boxes to the anchor boxes\n",
        "- The anchor boxes that are not assigned any objects, are either assigned the\n",
        "background class or ignored depending on the IOU\n",
        "- Generating the classification and regression targets using anchor boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "sWbx7xksu13Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LabelEncoder:\n",
        "    \"\"\"Transforms the raw labels into targets for training.\n",
        "\n",
        "    This class has operations to generate targets for a batch of samples which\n",
        "    is made up of the input images, bounding boxes for the objects present and\n",
        "    their class ids.\n",
        "\n",
        "    Attributes:\n",
        "      anchor_box: Anchor box generator to encode the bounding boxes.\n",
        "      box_variance: The scaling factors used to scale the bounding box targets.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self._anchor_box = AnchorBox()\n",
        "        self._box_variance = tf.convert_to_tensor(\n",
        "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
        "        )\n",
        "\n",
        "    def _match_anchor_boxes(\n",
        "        self, anchor_boxes, gt_boxes, match_iou=0.5, ignore_iou=0.4\n",
        "    ):\n",
        "        \"\"\"Matches ground truth boxes to anchor boxes based on IOU.\n",
        "\n",
        "        1. Calculates the pairwise IOU for the M `anchor_boxes` and N `gt_boxes`\n",
        "          to get a `(M, N)` shaped matrix.\n",
        "        2. The ground truth box with the maximum IOU in each row is assigned to\n",
        "          the anchor box provided the IOU is greater than `match_iou`.\n",
        "        3. If the maximum IOU in a row is less than `ignore_iou`, the anchor\n",
        "          box is assigned with the background class.\n",
        "        4. The remaining anchor boxes that do not have any class assigned are\n",
        "          ignored during training.\n",
        "\n",
        "        Arguments:\n",
        "          anchor_boxes: A float tensor with the shape `(total_anchors, 4)`\n",
        "            representing all the anchor boxes for a given input image shape,\n",
        "            where each anchor box is of the format `[x, y, width, height]`.\n",
        "          gt_boxes: A float tensor with shape `(num_objects, 4)` representing\n",
        "            the ground truth boxes, where each box is of the format\n",
        "            `[x, y, width, height]`.\n",
        "          match_iou: A float value representing the minimum IOU threshold for\n",
        "            determining if a ground truth box can be assigned to an anchor box.\n",
        "          ignore_iou: A float value representing the IOU threshold under which\n",
        "            an anchor box is assigned to the background class.\n",
        "\n",
        "        Returns:\n",
        "          matched_gt_idx: Index of the matched object\n",
        "          positive_mask: A mask for anchor boxes that have been assigned ground\n",
        "            truth boxes.\n",
        "          ignore_mask: A mask for anchor boxes that need to by ignored during\n",
        "            training\n",
        "        \"\"\"\n",
        "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n",
        "        max_iou = tf.reduce_max(iou_matrix, axis=1)\n",
        "        matched_gt_idx = tf.argmax(iou_matrix, axis=1)\n",
        "        positive_mask = tf.greater_equal(max_iou, match_iou)\n",
        "        negative_mask = tf.less(max_iou, ignore_iou)\n",
        "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))\n",
        "        return (\n",
        "            matched_gt_idx,\n",
        "            tf.cast(positive_mask, dtype=tf.float32),\n",
        "            tf.cast(ignore_mask, dtype=tf.float32),\n",
        "        )\n",
        "\n",
        "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
        "        \"\"\"Transforms the ground truth boxes into targets for training\"\"\"\n",
        "        box_target = tf.concat(\n",
        "            [\n",
        "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
        "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),\n",
        "            ],\n",
        "            axis=-1,\n",
        "        )\n",
        "        box_target = box_target / self._box_variance\n",
        "        return box_target\n",
        "\n",
        "    def _encode_sample(self, image_shape, gt_boxes, cls_ids):\n",
        "        \"\"\"Creates box and classification targets for a single sample\"\"\"\n",
        "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
        "        cls_ids = tf.cast(cls_ids, dtype=tf.float32)\n",
        "        matched_gt_idx, positive_mask, ignore_mask = self._match_anchor_boxes(\n",
        "            anchor_boxes, gt_boxes\n",
        "        )\n",
        "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
        "        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)\n",
        "        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
        "        cls_target = tf.where(\n",
        "            tf.not_equal(positive_mask, 1.0), -1.0, matched_gt_cls_ids\n",
        "        )\n",
        "        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -2.0, cls_target)\n",
        "        cls_target = tf.expand_dims(cls_target, axis=-1)\n",
        "        label = tf.concat([box_target, cls_target], axis=-1)\n",
        "        return label\n",
        "\n",
        "    def encode_batch(self, batch_images, gt_boxes, cls_ids):\n",
        "        \"\"\"Creates box and classification targets for a batch\"\"\"\n",
        "        images_shape = tf.shape(batch_images)\n",
        "        batch_size = images_shape[0]\n",
        "\n",
        "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
        "        for i in range(batch_size):\n",
        "            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n",
        "            labels = labels.write(i, label)\n",
        "        batch_images = tf.keras.applications.resnet.preprocess_input(batch_images)\n",
        "        return batch_images, labels.stack()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNF5kp7wu13a"
      },
      "source": [
        "## Building the ResNet50 backbone\n",
        "\n",
        "RetinaNet uses a ResNet based backbone, using which a feature pyramid network\n",
        "is constructed. In the example we use ResNet50 as the backbone, and return the\n",
        "feature maps at strides 8, 16 and 32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "AWiItfH9u13a"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_backbone():\n",
        "    \"\"\"Builds ResNet50 with pre-trained imagenet weights\"\"\"\n",
        "    backbone = keras.applications.ResNet50(\n",
        "        include_top=False, input_shape=[None, None, 3]\n",
        "    )\n",
        "    c3_output, c4_output, c5_output = [\n",
        "        backbone.get_layer(layer_name).output\n",
        "        for layer_name in [\"conv3_block4_out\", \"conv4_block6_out\", \"conv5_block3_out\"]\n",
        "    ]\n",
        "    return keras.Model(\n",
        "        inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output]\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5kAgO-Ju13b"
      },
      "source": [
        "## Building Feature Pyramid Network as a custom layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "A_E2bbsJu13b"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FeaturePyramid(keras.layers.Layer):\n",
        "    \"\"\"Builds the Feature Pyramid with the feature maps from the backbone.\n",
        "\n",
        "    Attributes:\n",
        "      num_classes: Number of classes in the dataset.\n",
        "      backbone: The backbone to build the feature pyramid from.\n",
        "        Currently supports ResNet50 only.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, backbone=None, **kwargs):\n",
        "        super().__init__(name=\"FeaturePyramid\", **kwargs)\n",
        "        self.backbone = backbone if backbone else get_backbone()\n",
        "        self.conv_c3_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c4_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c5_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c3_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c4_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c5_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c6_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n",
        "        self.conv_c7_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n",
        "        self.upsample_2x = keras.layers.UpSampling2D(2)\n",
        "\n",
        "    def call(self, images, training=False):\n",
        "        c3_output, c4_output, c5_output = self.backbone(images, training=training)\n",
        "        p3_output = self.conv_c3_1x1(c3_output)\n",
        "        p4_output = self.conv_c4_1x1(c4_output)\n",
        "        p5_output = self.conv_c5_1x1(c5_output)\n",
        "        p4_output = p4_output + self.upsample_2x(p5_output)\n",
        "        p3_output = p3_output + self.upsample_2x(p4_output)\n",
        "        p3_output = self.conv_c3_3x3(p3_output)\n",
        "        p4_output = self.conv_c4_3x3(p4_output)\n",
        "        p5_output = self.conv_c5_3x3(p5_output)\n",
        "        p6_output = self.conv_c6_3x3(c5_output)\n",
        "        p7_output = self.conv_c7_3x3(tf.nn.relu(p6_output))\n",
        "        return p3_output, p4_output, p5_output, p6_output, p7_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8UJFTy_u13c"
      },
      "source": [
        "## Building the classification and box regression heads.\n",
        "The RetinaNet model has separate heads for bounding box regression and\n",
        "for predicting class probabilities for the objects. These heads are shared\n",
        "between all the feature maps of the feature pyramid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-QlHmp1Wu13d"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_head(output_filters, bias_init):\n",
        "    \"\"\"Builds the class/box predictions head.\n",
        "\n",
        "    Arguments:\n",
        "      output_filters: Number of convolution filters in the final layer.\n",
        "      bias_init: Bias Initializer for the final convolution layer.\n",
        "\n",
        "    Returns:\n",
        "      A keras sequential model representing either the classification\n",
        "        or the box regression head depending on `output_filters`.\n",
        "    \"\"\"\n",
        "    head = keras.Sequential([keras.Input(shape=[None, None, 256])])\n",
        "    kernel_init = tf.initializers.RandomNormal(0.0, 0.01)\n",
        "    for _ in range(4):\n",
        "        head.add(\n",
        "            keras.layers.Conv2D(256, 3, padding=\"same\", kernel_initializer=kernel_init)\n",
        "        )\n",
        "        head.add(keras.layers.ReLU())\n",
        "    head.add(\n",
        "        keras.layers.Conv2D(\n",
        "            output_filters,\n",
        "            3,\n",
        "            1,\n",
        "            padding=\"same\",\n",
        "            kernel_initializer=kernel_init,\n",
        "            bias_initializer=bias_init,\n",
        "        )\n",
        "    )\n",
        "    return head\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_rk5o7lu13e"
      },
      "source": [
        "## Building RetinaNet using a subclassed model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "L7nh-bbGu13f"
      },
      "outputs": [],
      "source": [
        "\n",
        "class RetinaNet(keras.Model):\n",
        "    \"\"\"A subclassed Keras model implementing the RetinaNet architecture.\n",
        "\n",
        "    Attributes:\n",
        "      num_classes: Number of classes in the dataset.\n",
        "      backbone: The backbone to build the feature pyramid from.\n",
        "        Currently supports ResNet50 only.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, backbone=None, **kwargs):\n",
        "        super().__init__(name=\"RetinaNet\", **kwargs)\n",
        "        self.fpn = FeaturePyramid(backbone)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
        "        self.cls_head = build_head(9 * num_classes, prior_probability)\n",
        "        self.box_head = build_head(9 * 4, \"zeros\")\n",
        "\n",
        "    def call(self, image, training=False):\n",
        "        features = self.fpn(image, training=training)\n",
        "        N = tf.shape(image)[0]\n",
        "        cls_outputs = []\n",
        "        box_outputs = []\n",
        "        for feature in features:\n",
        "            box_outputs.append(tf.reshape(self.box_head(feature), [N, -1, 4]))\n",
        "            cls_outputs.append(\n",
        "                tf.reshape(self.cls_head(feature), [N, -1, self.num_classes])\n",
        "            )\n",
        "        cls_outputs = tf.concat(cls_outputs, axis=1)\n",
        "        box_outputs = tf.concat(box_outputs, axis=1)\n",
        "        return tf.concat([box_outputs, cls_outputs], axis=-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOw6ihwou13g"
      },
      "source": [
        "## Implementing a custom layer to decode predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "GLEPJ_nMu13h"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DecodePredictions(tf.keras.layers.Layer):\n",
        "    \"\"\"A Keras layer that decodes predictions of the RetinaNet model.\n",
        "\n",
        "    Attributes:\n",
        "      num_classes: Number of classes in the dataset\n",
        "      confidence_threshold: Minimum class probability, below which detections\n",
        "        are pruned.\n",
        "      nms_iou_threshold: IOU threshold for the NMS operation\n",
        "      max_detections_per_class: Maximum number of detections to retain per\n",
        "       class.\n",
        "      max_detections: Maximum number of detections to retain across all\n",
        "        classes.\n",
        "      box_variance: The scaling factors used to scale the bounding box\n",
        "        predictions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes=80,\n",
        "        confidence_threshold=0.05,\n",
        "        nms_iou_threshold=0.5,\n",
        "        max_detections_per_class=100,\n",
        "        max_detections=100,\n",
        "        box_variance=[0.1, 0.1, 0.2, 0.2],\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_classes = num_classes\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "        self.nms_iou_threshold = nms_iou_threshold\n",
        "        self.max_detections_per_class = max_detections_per_class\n",
        "        self.max_detections = max_detections\n",
        "\n",
        "        self._anchor_box = AnchorBox()\n",
        "        self._box_variance = tf.convert_to_tensor(\n",
        "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
        "        )\n",
        "\n",
        "    def _decode_box_predictions(self, anchor_boxes, box_predictions):\n",
        "        boxes = box_predictions * self._box_variance\n",
        "        boxes = tf.concat(\n",
        "            [\n",
        "                boxes[:, :, :2] * anchor_boxes[:, :, 2:] + anchor_boxes[:, :, :2],\n",
        "                tf.math.exp(boxes[:, :, 2:]) * anchor_boxes[:, :, 2:],\n",
        "            ],\n",
        "            axis=-1,\n",
        "        )\n",
        "        boxes_transformed = convert_to_corners(boxes)\n",
        "        return boxes_transformed\n",
        "\n",
        "    def call(self, images, predictions):\n",
        "        image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n",
        "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
        "        box_predictions = predictions[:, :, :4]\n",
        "        cls_predictions = tf.nn.sigmoid(predictions[:, :, 4:])\n",
        "        boxes = self._decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n",
        "\n",
        "        return tf.image.combined_non_max_suppression(\n",
        "            tf.expand_dims(boxes, axis=2),\n",
        "            cls_predictions,\n",
        "            self.max_detections_per_class,\n",
        "            self.max_detections,\n",
        "            self.nms_iou_threshold,\n",
        "            self.confidence_threshold,\n",
        "            clip_boxes=False,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzwBsPwru13i"
      },
      "source": [
        "## Implementing Smooth L1 loss and Focal Loss as keras custom losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "WwEz9ir3u13j"
      },
      "outputs": [],
      "source": [
        "\n",
        "class RetinaNetBoxLoss(tf.losses.Loss):\n",
        "    \"\"\"Implements Smooth L1 loss\"\"\"\n",
        "\n",
        "    def __init__(self, delta):\n",
        "        super().__init__(\n",
        "            reduction=\"none\", name=\"RetinaNetBoxLoss\"\n",
        "        )\n",
        "        self._delta = delta\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        difference = y_true - y_pred\n",
        "        absolute_difference = tf.abs(difference)\n",
        "        squared_difference = difference ** 2\n",
        "        loss = tf.where(\n",
        "            tf.less(absolute_difference, self._delta),\n",
        "            0.5 * squared_difference,\n",
        "            absolute_difference - 0.5,\n",
        "        )\n",
        "        return tf.reduce_sum(loss, axis=-1)\n",
        "\n",
        "\n",
        "class RetinaNetClassificationLoss(tf.losses.Loss):\n",
        "    \"\"\"Implements Focal loss\"\"\"\n",
        "\n",
        "    def __init__(self, alpha, gamma):\n",
        "        super().__init__(\n",
        "            reduction=\"none\", name=\"RetinaNetClassificationLoss\"\n",
        "        )\n",
        "        self._alpha = alpha\n",
        "        self._gamma = gamma\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            labels=y_true, logits=y_pred\n",
        "        )\n",
        "        probs = tf.nn.sigmoid(y_pred)\n",
        "        alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n",
        "        pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n",
        "        loss = alpha * tf.pow(1.0 - pt, self._gamma) * cross_entropy\n",
        "        return tf.reduce_sum(loss, axis=-1)\n",
        "\n",
        "\n",
        "class RetinaNetLoss(tf.losses.Loss):\n",
        "    \"\"\"Wrapper to combine both the losses\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=80, alpha=0.25, gamma=2.0, delta=1.0):\n",
        "        super().__init__(reduction=\"none\", name=\"RetinaNetLoss\")#\"auto\"\n",
        "        self._clf_loss = RetinaNetClassificationLoss(alpha, gamma)\n",
        "        self._box_loss = RetinaNetBoxLoss(delta)\n",
        "        self._num_classes = num_classes\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
        "        box_labels = y_true[:, :, :4]\n",
        "        box_predictions = y_pred[:, :, :4]\n",
        "        cls_labels = tf.one_hot(\n",
        "            tf.cast(y_true[:, :, 4], dtype=tf.int32),\n",
        "            depth=self._num_classes,\n",
        "            dtype=tf.float32,\n",
        "        )\n",
        "        cls_predictions = y_pred[:, :, 4:]\n",
        "        positive_mask = tf.cast(tf.greater(y_true[:, :, 4], -1.0), dtype=tf.float32)\n",
        "        ignore_mask = tf.cast(tf.equal(y_true[:, :, 4], -2.0), dtype=tf.float32)\n",
        "        clf_loss = self._clf_loss(cls_labels, cls_predictions)\n",
        "        box_loss = self._box_loss(box_labels, box_predictions)\n",
        "        clf_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, clf_loss)\n",
        "        box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)\n",
        "        normalizer = tf.reduce_sum(positive_mask, axis=-1)\n",
        "        clf_loss = tf.math.divide_no_nan(tf.reduce_sum(clf_loss, axis=-1), normalizer)\n",
        "        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n",
        "        loss = clf_loss + box_loss\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOOOU1tOu13k"
      },
      "source": [
        "## Setting up training parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "rs7QDbf6u13k"
      },
      "outputs": [],
      "source": [
        "model_dir = \"retinanet/\"\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "num_classes = 80\n",
        "batch_size = 2\n",
        "\n",
        "learning_rates = [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\n",
        "learning_rate_boundaries = [125, 250, 500, 240000, 360000]\n",
        "learning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    boundaries=learning_rate_boundaries, values=learning_rates\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDBdC1k-u13l"
      },
      "source": [
        "## Initializing and compiling model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "gDz759huu13l"
      },
      "outputs": [],
      "source": [
        "resnet50_backbone = get_backbone()\n",
        "loss_fn = RetinaNetLoss(num_classes)\n",
        "model = RetinaNet(num_classes, resnet50_backbone)\n",
        "#legacy\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate_fn, momentum=0.9)\n",
        "model.compile(loss=loss_fn, optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jp4A4d2u13m"
      },
      "source": [
        "## Setting up callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "k2F7whZku13m"
      },
      "outputs": [],
      "source": [
        "callbacks_list = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(model_dir, \"epoch_{epoch}_.weights.h5\"),\n",
        "        monitor=\"loss\",\n",
        "        save_best_only=False,\n",
        "        save_weights_only=True,\n",
        "        verbose=1,\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf3ORRpwu13n"
      },
      "source": [
        "## Load the COCO2017 dataset using TensorFlow Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "sxybGRFyu13n"
      },
      "outputs": [],
      "source": [
        "#  set `data_dir=None` to load the complete dataset\n",
        "\n",
        "(train_dataset, val_dataset), dataset_info = tfds.load(\n",
        "    \"coco/2017\", split=[\"train\", \"validation\"], with_info=True, data_dir=\"data\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVolI13Lu13n"
      },
      "source": [
        "## Setting up a `tf.data` pipeline\n",
        "\n",
        "To ensure that the model is fed with data efficiently we will be using\n",
        "`tf.data` API to create our input pipeline. The input pipeline\n",
        "consists for the following major processing steps:\n",
        "\n",
        "- Apply the preprocessing function to the samples\n",
        "- Create batches with fixed batch size. Since images in the batch can\n",
        "have different dimensions, and can also have different number of\n",
        "objects, we use `padded_batch` to the add the necessary padding to create\n",
        "rectangular tensors\n",
        "- Create targets for each sample in the batch using `LabelEncoder`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "adpKU6fcu13o"
      },
      "outputs": [],
      "source": [
        "autotune = tf.data.AUTOTUNE\n",
        "train_dataset = train_dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
        "train_dataset = train_dataset.shuffle(8 * batch_size)\n",
        "train_dataset = train_dataset.padded_batch(\n",
        "    batch_size=batch_size, padding_values=(0.0, 1e-8, -1), drop_remainder=True\n",
        ")\n",
        "train_dataset = train_dataset.map(\n",
        "    label_encoder.encode_batch, num_parallel_calls=autotune\n",
        ")\n",
        "train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())\n",
        "train_dataset = train_dataset.prefetch(autotune)\n",
        "\n",
        "val_dataset = val_dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
        "val_dataset = val_dataset.padded_batch(\n",
        "    batch_size=1, padding_values=(0.0, 1e-8, -1), drop_remainder=True\n",
        ")\n",
        "val_dataset = val_dataset.map(label_encoder.encode_batch, num_parallel_calls=autotune)\n",
        "val_dataset = val_dataset.apply(tf.data.experimental.ignore_errors())\n",
        "val_dataset = val_dataset.prefetch(autotune)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtwuDCm-u13o"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjWE9rwbu13o",
        "outputId": "8e2a20e9-3afe-49c1-c6fb-ee8602615aba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    100/Unknown \u001b[1m1426s\u001b[0m 13s/step - loss: 4.0573"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: saving model to retinanet/epoch_1_.weights.h5\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1608s\u001b[0m 15s/step - loss: 4.0574 - val_loss: 4.0953\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fed6bc0dff0>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# Uncomment the following lines, when training on full dataset\n",
        "# train_steps_per_epoch = dataset_info.splits[\"train\"].num_examples // batch_size\n",
        "# val_steps_per_epoch = \\\n",
        "#     dataset_info.splits[\"validation\"].num_examples // batch_size\n",
        "\n",
        "# train_steps = 4 * 100000\n",
        "# epochs = train_steps // train_steps_per_epoch\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "# Running 100 training and 50 validation steps,\n",
        "# remove `.take` when training on the full dataset\n",
        "\n",
        "model.fit(\n",
        "    train_dataset.take(100),\n",
        "    validation_data=val_dataset.take(50),\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks_list,\n",
        "    verbose=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()\n",
        "print(os.listdir('/content/retinanet'))\n",
        "with open('/content/retinanet/epoch_1_.weights.h5', 'rb') as f:\n",
        "    str1 = f.read()\n",
        "print(len(str1))\n",
        "help(tf.train.latest_checkpoint)"
      ],
      "metadata": {
        "id": "svoq63zZF9RM",
        "outputId": "2585f96b-b13b-4614-e624-011bb45b4c0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['epoch_1_.weights.h5']\n",
            "304792888\n",
            "Help on function latest_checkpoint in module tensorflow.python.checkpoint.checkpoint_management:\n",
            "\n",
            "latest_checkpoint(checkpoint_dir, latest_filename=None)\n",
            "    Finds the filename of latest saved checkpoint file.\n",
            "    \n",
            "    Gets the checkpoint state given the provided checkpoint_dir and looks for a\n",
            "    corresponding TensorFlow 2 (preferred) or TensorFlow 1.x checkpoint path.\n",
            "    The latest_filename argument is only applicable if you are saving checkpoint\n",
            "    using `v1.train.Saver.save`\n",
            "    \n",
            "    \n",
            "    See the [Training Checkpoints\n",
            "    Guide](https://www.tensorflow.org/guide/checkpoint) for more details and\n",
            "    examples.`\n",
            "    \n",
            "    Args:\n",
            "      checkpoint_dir: Directory where the variables were saved.\n",
            "      latest_filename: Optional name for the protocol buffer file that\n",
            "        contains the list of most recent checkpoint filenames.\n",
            "        See the corresponding argument to `v1.train.Saver.save`.\n",
            "    \n",
            "    Returns:\n",
            "      The full path to the latest checkpoint or `None` if no checkpoint was found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV5gRwm2u13o"
      },
      "source": [
        "## Loading weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "WkpV-5TMu13p",
        "outputId": "1fd2ac45-7170-4c49-fac4-e4841c13ceec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['epoch_1_.weights.h5']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-56523e2b7958>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mweights_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/retinanet'\u001b[0m\u001b[0;31m#\"data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlatest_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatest_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'epoch_1_.weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/retinanet/epoch_1_.weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#latest_checkpoint)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/checkpoint/checkpoint_management.py\u001b[0m in \u001b[0;36mlatest_checkpoint\u001b[0;34m(checkpoint_dir, latest_filename)\u001b[0m\n\u001b[1;32m    350\u001b[0m   \"\"\"\n\u001b[1;32m    351\u001b[0m   \u001b[0;31m# Pick the latest checkpoint based on checkpoint state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m   \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_checkpoint_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatest_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mckpt\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;31m# Look for either a V2 path or a V1 path, with priority for V2.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/checkpoint/checkpoint_management.py\u001b[0m in \u001b[0;36mget_checkpoint_state\u001b[0;34m(checkpoint_dir, latest_filename)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;31m# many lines of errors from colossus in the logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord_checkpoint_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m       file_content = file_io.read_file_to_string(\n\u001b[0m\u001b[1;32m    280\u001b[0m           coord_checkpoint_filename)\n\u001b[1;32m    281\u001b[0m       \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCheckpointState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mread_file_to_string\u001b[0;34m(filename, binary_mode)\u001b[0m\n\u001b[1;32m    349\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m       \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   @deprecation.deprecated_args(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m_prepare_value\u001b[0;34m(self, val)\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_str_any\u001b[0;34m(value, encoding)\u001b[0m\n\u001b[1;32m    132\u001b[0m   \"\"\"\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_str\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_or_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_or_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0mtf_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'compat.as_text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_text\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_or_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected binary or unicode string, got %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte"
          ]
        }
      ],
      "source": [
        "# Change this to `model_dir` when not using the downloaded weights\n",
        "weights_dir = '/content/retinanet'#\"data\"\n",
        "print(os.listdir(weights_dir))\n",
        "latest_checkpoint = tf.train.latest_checkpoint(weights_dir, latest_filename = 'epoch_1_.weights.h5')\n",
        "model.load_weights('/content/retinanet/epoch_1_.weights.h5')#latest_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3oEDjuDu13p"
      },
      "source": [
        "## Building inference model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "5jvcI5p5u13r"
      },
      "outputs": [],
      "source": [
        "image = tf.keras.Input(shape=[None, None, 3], name=\"image\")\n",
        "predictions = model(image, training=False)\n",
        "detections = DecodePredictions(confidence_threshold=0.5)(image, predictions)\n",
        "inference_model = tf.keras.Model(inputs=image, outputs=detections)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRqxx5o4u13s"
      },
      "source": [
        "## Generating detections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J7znao58u13t",
        "outputId": "5a5df44c-7d9b-4325-ccf7-1a85e0d55719"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node functional_39_1/decode_predictions_1/combined_non_max_suppression/CombinedNonMaxSuppression defined at (most recent call last):\n<stack traces unavailable>\nDetected at node functional_39_1/decode_predictions_1/combined_non_max_suppression/CombinedNonMaxSuppression defined at (most recent call last):\n<stack traces unavailable>\nDetected unsupported operations when trying to compile graph __inference_one_step_on_data_63755[] on XLA_GPU_JIT: CombinedNonMaxSuppression (No registered 'CombinedNonMaxSuppression' OpKernel for XLA_GPU_JIT devices compatible with node {{node functional_39_1/decode_predictions_1/combined_non_max_suppression/CombinedNonMaxSuppression}}){{node functional_39_1/decode_predictions_1/combined_non_max_suppression/CombinedNonMaxSuppression}}\nThe op is created at: \nFile \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\nFile \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\nFile \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\nFile \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\nFile \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\nFile \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\nFile \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\nFile \"<ipython-input-70-ff873dd721ac>\", line 13, in <cell line: 10>\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 512, in predict\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 208, in one_step_on_data_distributed\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 198, in one_step_on_data\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 96, in predict_step\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py\", line 901, in __call__\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py\", line 46, in __call__\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/models/functional.py\", line 175, in call\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/ops/function.py\", line 171, in _run_through_graph\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/models/functional.py\", line 560, in call\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py\", line 901, in __call__\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py\", line 46, in __call__\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\nFile \"<ipython-input-28-5aaa2d194c9d>\", line 58, in call\n\ttf2xla conversion failed while converting __inference_one_step_on_data_63755[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n\t [[StatefulPartitionedCall]] [Op:__inference_one_step_on_data_distributed_64484]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-ff873dd721ac>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0minput_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mnum_detections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_detections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     class_names = [\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node functional_39_1/decode_predictions_1/combined_non_max_suppression/CombinedNonMaxSuppression defined at (most recent call last):\n<stack traces unavailable>\nDetected at node functional_39_1/decode_predictions_1/combined_non_max_suppression/CombinedNonMaxSuppression defined at (most recent call last):\n<stack traces unavailable>\nDetected unsupported operations when trying to compile graph __inference_one_step_on_data_63755[] on XLA_GPU_JIT: CombinedNonMaxSuppression (No registered 'CombinedNonMaxSuppression' OpKernel for XLA_GPU_JIT devices compatible with node {{node functional_39_1/decode_predictions_1/combined_non_max_suppression/CombinedNonMaxSuppression}}){{node functional_39_1/decode_predictions_1/combined_non_max_suppression/CombinedNonMaxSuppression}}\nThe op is created at: \nFile \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\nFile \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\nFile \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\nFile \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\nFile \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\nFile \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\nFile \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\nFile \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\nFile \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\nFile \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\nFile \"<ipython-input-70-ff873dd721ac>\", line 13, in <cell line: 10>\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 512, in predict\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 208, in one_step_on_data_distributed\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 198, in one_step_on_data\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 96, in predict_step\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py\", line 901, in __call__\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py\", line 46, in __call__\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/models/functional.py\", line 175, in call\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/ops/function.py\", line 171, in _run_through_graph\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/models/functional.py\", line 560, in call\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py\", line 901, in __call__\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py\", line 46, in __call__\nFile \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\nFile \"<ipython-input-28-5aaa2d194c9d>\", line 58, in call\n\ttf2xla conversion failed while converting __inference_one_step_on_data_63755[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n\t [[StatefulPartitionedCall]] [Op:__inference_one_step_on_data_distributed_64484]"
          ]
        }
      ],
      "source": [
        "\n",
        "def prepare_image(image):\n",
        "    image, _, ratio = resize_and_pad_image(image, jitter=None)\n",
        "    image = tf.keras.applications.resnet.preprocess_input(image)\n",
        "    return tf.expand_dims(image, axis=0), ratio\n",
        "\n",
        "\n",
        "val_dataset = tfds.load(\"coco/2017\", split=\"validation\", data_dir=\"data\")\n",
        "int2str = dataset_info.features[\"objects\"][\"label\"].int2str\n",
        "\n",
        "for sample in val_dataset.take(2):\n",
        "    image = tf.cast(sample[\"image\"], dtype=tf.float32)\n",
        "    input_image, ratio = prepare_image(image)\n",
        "    detections = inference_model.predict(input_image)\n",
        "    num_detections = detections.valid_detections[0]\n",
        "    class_names = [\n",
        "        int2str(int(x)) for x in detections.nmsed_classes[0][:num_detections]\n",
        "    ]\n",
        "    visualize_detections(\n",
        "        image,\n",
        "        detections.nmsed_boxes[0][:num_detections] / ratio,\n",
        "        class_names,\n",
        "        detections.nmsed_scores[0][:num_detections],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1xqoD03u13u"
      },
      "source": [
        "Example available on HuggingFace.\n",
        "\n",
        "| Trained Model | Demo |\n",
        "| :--: | :--: |\n",
        "| [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Model-Object%20Detection%20With%20Retinanet-black.svg)](https://huggingface.co/keras-io/Object-Detection-RetinaNet) | [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-Object%20Detection%20With%20Retinanet-black.svg)](https://huggingface.co/spaces/keras-io/Object-Detection-Using-RetinaNet) |"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "retinanet",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}